version: '3.2'

services:
  # https://hub.docker.com/r/confluentinc/cp-zookeeper/
  zookeeper:
    image: confluentinc/cp-zookeeper:5.1.2
    container_name: zookeeper
    hostname: zookeeper
    restart: always
    networks:
      - kafka-net
    deploy:
      placement:
        # set node labels using docker node update --label-add key=value <NODE ID> from swarm manager
        constraints:
          - node.labels.zookeeper==1
    env_file:
      - .env
    healthcheck:
      test: echo srvr | nc zookeeper 2181 || exit 1
      retries: 20
      interval: 10s
    ports:
      - $ZOOKEPER__PORT:2181
    environment:
      - ZOOKEEPER_CLIENT_PORT=$ZOOKEPER__PORT
      - TZ=$TZ
      
  # https://hub.docker.com/r/confluentinc/cp-kafka/
  kafka1:
    image: confluentinc/cp-kafka:5.1.2
    container_name: kafka1
    ports:
      - $KAFKA__PORT_1:$KAFKA__PORT_1
    environment:
      KAFKA_LISTENERS: PLAINTEXT://:19092,LISTENER_DOCKER_EXTERNAL://kafka1:$KAFKA__PORT_1
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://:19092,LISTENER_DOCKER_EXTERNAL://kafka1:$KAFKA__PORT_1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:$ZOOKEPER__PORT"
      KAFKA_BROKER_ID: 1
      KAFKA_NUM_PARTITIONS: $KAFKA__NUM_PARTITIONS
      KAFKA_DEFAULT_REPLICATION_FACTOR: $KAFKA__NUM_REPLICAS_CLUSTER
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'
    restart: always
    networks:
      - kafka-net
    deploy:
      restart_policy:
        condition: on-failure
      placement:
        # set node labels using docker node update --label-add key=value <NODE ID> from swarm manager
        constraints:
          - node.labels.kafka==1
      replicas: 1
    volumes:
      # directory for logs
      - $KAFKA__DATA:/var/lib/kafka/data
      # directory of configs
      - $KAFKA__CONFIGS:/etc/kafka
    depends_on:
      - zookeeper
    command: >
          sh -c "((sleep 15 && \
                /usr/bin/kafka-topics \
                --zookeeper zookeeper:$ZOOKEPER__PORT \
                --create --topic $KAFKA__TOPIC \
                --partitions $KAFKA__NUM_PARTITIONS \
                --replication-factor $KAFKA__NUM_REPLICAS_CLUSTER)&) && \
                /etc/confluent/docker/run "
  
  kafka2:
    image: confluentinc/cp-kafka:5.1.2
    container_name: kafka2
    ports:
      - $KAFKA__PORT_2:$KAFKA__PORT_2
    environment:
      KAFKA_LISTENERS: PLAINTEXT://:19092,LISTENER_DOCKER_EXTERNAL://kafka2:$KAFKA__PORT_2
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://:19092,LISTENER_DOCKER_EXTERNAL://kafka2:$KAFKA__PORT_2
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:$ZOOKEPER__PORT"
      KAFKA_BROKER_ID: 2
      KAFKA_NUM_PARTITIONS: $KAFKA__NUM_PARTITIONS
      KAFKA_DEFAULT_REPLICATION_FACTOR: $KAFKA__NUM_REPLICAS_CLUSTER
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'
    restart: always
    networks:
      - kafka-net
    deploy:
      restart_policy:
        condition: on-failure
      placement:
        # set node labels using docker node update --label-add key=value <NODE ID> from swarm manager
        constraints:
          - node.labels.kafka==2
      replicas: 1
    volumes:
      # directory for logs
      - $KAFKA__DATA:/var/lib/kafka/data
      # directory of configs
      - $KAFKA__CONFIGS:/etc/kafka
    depends_on:
      - zookeeper
    command: >
          sh -c "((sleep 15 && \
                /usr/bin/kafka-topics \
                --zookeeper zookeeper:$ZOOKEPER__PORT \
                --create --topic $KAFKA__TOPIC \
                --partitions $KAFKA__NUM_PARTITIONS \
                --replication-factor $KAFKA__NUM_REPLICAS_CLUSTER)&) && \
                /etc/confluent/docker/run "
  
  kafka3:
    image: confluentinc/cp-kafka:5.1.2
    container_name: kafka3
    ports:
      - $KAFKA__PORT_3:$KAFKA__PORT_3
    environment:
      KAFKA_LISTENERS: PLAINTEXT://:19092,LISTENER_DOCKER_EXTERNAL://kafka3:$KAFKA__PORT_1
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://:19092,LISTENER_DOCKER_EXTERNAL://kafka3:$KAFKA__PORT_1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:$ZOOKEPER__PORT"
      KAFKA_BROKER_ID: 3
      KAFKA_NUM_PARTITIONS: $KAFKA__NUM_PARTITIONS
      KAFKA_DEFAULT_REPLICATION_FACTOR: $KAFKA__NUM_REPLICAS_CLUSTER
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'
    restart: always
    networks:
      - kafka-net
    deploy:
      restart_policy:
        condition: on-failure
      placement:
        # set node labels using docker node update --label-add key=value <NODE ID> from swarm manager
        constraints:
          - node.labels.kafka==3
      replicas: 1
    volumes:
      # directory for logs
      - $KAFKA__DATA:/var/lib/kafka/data
      # directory of configs
      - $KAFKA__CONFIGS:/etc/kafka
    depends_on:
      - zookeeper
    command: >
          sh -c "((sleep 15 && \
                /usr/bin/kafka-topics \
                --zookeeper zookeeper:$ZOOKEPER__PORT \
                --create --topic $KAFKA__TOPIC \
                --partitions $KAFKA__NUM_PARTITIONS \
                --replication-factor $KAFKA__NUM_REPLICAS_CLUSTER)&) && \
                /etc/confluent/docker/run "
  
networks:
  kafka-net:
    driver: overlay